{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using poppler path: /opt/homebrew/bin\n",
      "Using tesseract path: /opt/homebrew/bin/tesseract\n",
      "Length of extracted text: 2876\n",
      "Number of lines: 46\n",
      "Line: 'Mercifully, by the next morning my breathing had rebounded to the point where'\n",
      "Line: 'the doctors felt comfortable releasing me from the coma. When I finally'\n",
      "Line: 'regained consciousness, I discovered that I had lost my ability to smell. As a test,'\n",
      "Line: 'a nurse asked me to blow my nose and sniff an apple juice box. My sense of'\n",
      "Line: 'smell returned, but—to everyone’s surprise—the act of blowing my nose forced'\n",
      "Line: 'air through the fractures in my eye socket and pushed my left eye outward. My'\n",
      "Line: 'eyeball bulged out of the socket, held precariously in place by my eyelid and the'\n",
      "Line: 'optic nerve attaching my eye to my brain.'\n",
      "Line: ''\n",
      "Line: 'The ophthalmologist said my eye would gradually slide back into place as the'\n",
      "Line: 'air seeped out, but it was hard to tell how long this would take. I was scheduled'\n",
      "Line: 'for surgery one week later, which would allow me some additional time to heal.'\n",
      "Line: 'I looked like I had been on the wrong end of a boxing match, but I was cleared'\n",
      "Line: 'to leave the hospital. I returned home with a broken nose, half a dozen facial'\n",
      "Line: 'fractures, and a bulging left eye.'\n",
      "Line: ''\n",
      "Line: 'The following months were hard. It felt like everything in my life was on'\n",
      "Line: 'pause. I had double vision for weeks; I literally couldn’t see straight. It took'\n",
      "Line: 'more than a month, but my eyeball did eventually return to its normal location.'\n",
      "Line: 'Between the seizures and my vision problems, it was eight months before I could'\n",
      "Line: 'drive a car again. At physical therapy, I practiced basic motor patterns like'\n",
      "Line: 'walking in a straight line. I was determined not to let my injury get me down, but'\n",
      "Line: 'there were more than a few moments when I felt depressed and overwhelmed.'\n",
      "Line: ''\n",
      "Line: 'I became painfully aware of how far I had to go when I returned to the'\n",
      "Line: 'baseball field one year later. Baseball had always been a major part of my life.'\n",
      "Line: 'My dad had played minor league baseball for the St. Louis Cardinals, and I had a'\n",
      "Line: 'dream of playing professionally, too. After months of rehabilitation, what I'\n",
      "Line: 'wanted more than anything was to get back on the field.'\n",
      "Line: ''\n",
      "Line: 'But my return to baseball was not smooth. When the season rolled around, I'\n",
      "Line: 'was the only junior to be cut from the varsity baseball team. I was sent down to'\n",
      "Line: 'play with the sophomores on junior varsity. I had been playing since age four,'\n",
      "Line: 'and for someone who had spent so much time and effort on the sport, getting cut'\n",
      "Line: 'was humiliating. I vividly remember the day it happened. I sat in my car and'\n",
      "Line: 'cried as I flipped through the radio, desperately searching for a song that would'\n",
      "Line: 'make me feel better.'\n",
      "Line: ''\n",
      "Line: 'After a year of self-doubt, I managed to make the varsity team as a senior, but'\n",
      "Line: 'I rarely made it on the field. In total, I played eleven innings of high school'\n",
      "Line: 'varsity baseball, barely more than a single game.'\n",
      "Line: ''\n",
      "Line: 'Despite my lackluster high school career, I still believed I could become a'\n",
      "Line: 'great player. And I knew that if things were going to improve, I was the one'\n",
      "Line: 'responsible for making it happen. The turning point came two years after my'\n",
      "Line: ''\n"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # Get the poppler path\n",
    "    if os.path.exists('/opt/homebrew/bin'):  # For Apple Silicon Macs\n",
    "        poppler_path = '/opt/homebrew/bin'\n",
    "        pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/bin/tesseract'\n",
    "    elif os.path.exists('/usr/local/bin'):   # For Intel Macs\n",
    "        poppler_path = '/usr/local/bin'\n",
    "        pytesseract.pytesseract.tesseract_cmd = '/usr/local/bin/tesseract'\n",
    "    else:\n",
    "        raise Exception(\"Couldn't find required paths\")\n",
    "        \n",
    "    print(f\"Using poppler path: {poppler_path}\")\n",
    "    print(f\"Using tesseract path: {pytesseract.pytesseract.tesseract_cmd}\")\n",
    "    \n",
    "    # Convert PDF to images\n",
    "    pages = convert_from_path(\n",
    "        'Atomic habits.pdf',\n",
    "        first_page=10,\n",
    "        last_page=10,\n",
    "        poppler_path=poppler_path\n",
    "    )\n",
    "    \n",
    "    # Get first page as image\n",
    "    first_page = pages[0]\n",
    "    \n",
    "    # Extract text using OCR\n",
    "    text = pytesseract.image_to_string(first_page)\n",
    "    \n",
    "    # Debug info\n",
    "    print(f\"Length of extracted text: {len(text)}\")\n",
    "    \n",
    "    # Split and print first 10 lines\n",
    "    lines = text.split('\\n')\n",
    "    print(f\"Number of lines: {len(lines)}\")\n",
    "    \n",
    "    for line in lines[:100]:\n",
    "        print(f\"Line: '{line}'\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Could not find 'Atomic habits.pdf'\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages found: 256\n",
      "Processed page 1/256\n",
      "Processed page 2/256\n",
      "Processed page 3/256\n",
      "Processed page 4/256\n",
      "Processed page 5/256\n",
      "Processed page 6/256\n",
      "Processed page 7/256\n",
      "Processed page 8/256\n",
      "Processed page 9/256\n",
      "Processed page 10/256\n",
      "Processed page 11/256\n",
      "Processed page 12/256\n",
      "Processed page 13/256\n",
      "Processed page 14/256\n",
      "Processed page 15/256\n",
      "Processed page 16/256\n",
      "Processed page 17/256\n",
      "Processed page 18/256\n",
      "Processed page 19/256\n",
      "Processed page 20/256\n",
      "Processed page 21/256\n",
      "Processed page 22/256\n",
      "Processed page 23/256\n",
      "Processed page 24/256\n",
      "Processed page 25/256\n",
      "Processed page 26/256\n",
      "Processed page 27/256\n",
      "Processed page 28/256\n",
      "Processed page 29/256\n",
      "Processed page 30/256\n",
      "Processed page 31/256\n",
      "Processed page 32/256\n",
      "Processed page 33/256\n",
      "Processed page 34/256\n",
      "Processed page 35/256\n",
      "Processed page 36/256\n",
      "Processed page 37/256\n",
      "Processed page 38/256\n",
      "Processed page 39/256\n",
      "Processed page 40/256\n",
      "Processed page 41/256\n",
      "Processed page 42/256\n",
      "Processed page 43/256\n",
      "Processed page 44/256\n",
      "Processed page 45/256\n",
      "Processed page 46/256\n",
      "Processed page 47/256\n",
      "Processed page 48/256\n",
      "Processed page 49/256\n",
      "Processed page 50/256\n",
      "Processed page 51/256\n",
      "Processed page 52/256\n",
      "Processed page 53/256\n",
      "Processed page 54/256\n",
      "Processed page 55/256\n",
      "Processed page 56/256\n",
      "Processed page 57/256\n",
      "Processed page 58/256\n",
      "Processed page 59/256\n",
      "Processed page 60/256\n",
      "Processed page 61/256\n",
      "Processed page 62/256\n",
      "Processed page 63/256\n",
      "Processed page 64/256\n",
      "Processed page 65/256\n",
      "Processed page 66/256\n",
      "Processed page 67/256\n",
      "Processed page 68/256\n",
      "Processed page 69/256\n",
      "Processed page 70/256\n",
      "Processed page 71/256\n",
      "Processed page 72/256\n",
      "Processed page 73/256\n",
      "Processed page 74/256\n",
      "Processed page 75/256\n",
      "Processed page 76/256\n",
      "Processed page 77/256\n",
      "Processed page 78/256\n",
      "Processed page 79/256\n",
      "Processed page 80/256\n",
      "Processed page 81/256\n",
      "Processed page 82/256\n",
      "Processed page 83/256\n",
      "Processed page 84/256\n",
      "Processed page 85/256\n",
      "Processed page 86/256\n",
      "Processed page 87/256\n",
      "Processed page 88/256\n",
      "Processed page 89/256\n",
      "Processed page 90/256\n",
      "Processed page 91/256\n",
      "Processed page 92/256\n",
      "Processed page 93/256\n",
      "Processed page 94/256\n",
      "Processed page 95/256\n",
      "Processed page 96/256\n",
      "Processed page 97/256\n",
      "Processed page 98/256\n",
      "Processed page 99/256\n",
      "Processed page 100/256\n",
      "Processed page 101/256\n",
      "Processed page 102/256\n",
      "Processed page 103/256\n",
      "Processed page 104/256\n",
      "Processed page 105/256\n",
      "Processed page 106/256\n",
      "Processed page 107/256\n",
      "Processed page 108/256\n",
      "Processed page 109/256\n",
      "Processed page 110/256\n",
      "Processed page 111/256\n",
      "Processed page 112/256\n",
      "Processed page 113/256\n",
      "Processed page 114/256\n",
      "Processed page 115/256\n",
      "Processed page 116/256\n",
      "Processed page 117/256\n",
      "Processed page 118/256\n",
      "Processed page 119/256\n",
      "Processed page 120/256\n",
      "Processed page 121/256\n",
      "Processed page 122/256\n",
      "Processed page 123/256\n",
      "Processed page 124/256\n",
      "Processed page 125/256\n",
      "Processed page 126/256\n",
      "Processed page 127/256\n",
      "Processed page 128/256\n",
      "Processed page 129/256\n",
      "Processed page 130/256\n",
      "Processed page 131/256\n",
      "Processed page 132/256\n",
      "Processed page 133/256\n",
      "Processed page 134/256\n",
      "Processed page 135/256\n",
      "Processed page 136/256\n",
      "Processed page 137/256\n",
      "Processed page 138/256\n",
      "Processed page 139/256\n",
      "Processed page 140/256\n",
      "Processed page 141/256\n",
      "Processed page 142/256\n",
      "Processed page 143/256\n",
      "Processed page 144/256\n",
      "Processed page 145/256\n",
      "Processed page 146/256\n",
      "Processed page 147/256\n",
      "Processed page 148/256\n",
      "Processed page 149/256\n",
      "Processed page 150/256\n",
      "Processed page 151/256\n",
      "Processed page 152/256\n",
      "Processed page 153/256\n",
      "Processed page 154/256\n",
      "Processed page 155/256\n",
      "Processed page 156/256\n",
      "Processed page 157/256\n",
      "Processed page 158/256\n",
      "Processed page 159/256\n",
      "Processed page 160/256\n",
      "Processed page 161/256\n",
      "Processed page 162/256\n",
      "Processed page 163/256\n",
      "Processed page 164/256\n",
      "Processed page 165/256\n",
      "Processed page 166/256\n",
      "Processed page 167/256\n",
      "Processed page 168/256\n",
      "Processed page 169/256\n",
      "Processed page 170/256\n",
      "Processed page 171/256\n",
      "Processed page 172/256\n",
      "Processed page 173/256\n",
      "Processed page 174/256\n",
      "Processed page 175/256\n",
      "Processed page 176/256\n",
      "Processed page 177/256\n",
      "Processed page 178/256\n",
      "Processed page 179/256\n",
      "Processed page 180/256\n",
      "Processed page 181/256\n",
      "Processed page 182/256\n",
      "Processed page 183/256\n",
      "Processed page 184/256\n",
      "Processed page 185/256\n",
      "Processed page 186/256\n",
      "Processed page 187/256\n",
      "Processed page 188/256\n",
      "Processed page 189/256\n",
      "Processed page 190/256\n",
      "Processed page 191/256\n",
      "Processed page 192/256\n",
      "Processed page 193/256\n",
      "Processed page 194/256\n",
      "Processed page 195/256\n",
      "Processed page 196/256\n",
      "Processed page 197/256\n",
      "Processed page 198/256\n",
      "Processed page 199/256\n",
      "Processed page 200/256\n",
      "Processed page 201/256\n",
      "Processed page 202/256\n",
      "Processed page 203/256\n",
      "Processed page 204/256\n",
      "Processed page 205/256\n",
      "Processed page 206/256\n",
      "Processed page 207/256\n",
      "Processed page 208/256\n",
      "Processed page 209/256\n",
      "Processed page 210/256\n",
      "Processed page 211/256\n",
      "Processed page 212/256\n",
      "Processed page 213/256\n",
      "Processed page 214/256\n",
      "Processed page 215/256\n",
      "Processed page 216/256\n",
      "Processed page 217/256\n",
      "Processed page 218/256\n",
      "Processed page 219/256\n",
      "Processed page 220/256\n",
      "Processed page 221/256\n",
      "Processed page 222/256\n",
      "Processed page 223/256\n",
      "Processed page 224/256\n",
      "Processed page 225/256\n",
      "Processed page 226/256\n",
      "Processed page 227/256\n",
      "Processed page 228/256\n",
      "Processed page 229/256\n",
      "Processed page 230/256\n",
      "Processed page 231/256\n",
      "Processed page 232/256\n",
      "Processed page 233/256\n",
      "Processed page 234/256\n",
      "Processed page 235/256\n",
      "Processed page 236/256\n",
      "Processed page 237/256\n",
      "Processed page 238/256\n",
      "Processed page 239/256\n",
      "Processed page 240/256\n",
      "Processed page 241/256\n",
      "Processed page 242/256\n",
      "Processed page 243/256\n",
      "Processed page 244/256\n",
      "Processed page 245/256\n",
      "Processed page 246/256\n",
      "Processed page 247/256\n",
      "Processed page 248/256\n",
      "Processed page 249/256\n",
      "Processed page 250/256\n",
      "Processed page 251/256\n",
      "Processed page 252/256\n",
      "Processed page 253/256\n",
      "Processed page 254/256\n",
      "Processed page 255/256\n",
      "Processed page 256/256\n",
      "Successfully saved text to atomic_habits.txt\n",
      "Total characters extracted: 422715\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Convert all pages of PDF to images\n",
    "    pages = convert_from_path(\n",
    "        'Atomic habits.pdf',\n",
    "        poppler_path=poppler_path\n",
    "    )\n",
    "    \n",
    "    print(f\"Total pages found: {len(pages)}\")\n",
    "    \n",
    "    # Extract text from all pages\n",
    "    full_text = \"\"\n",
    "    for i, page in enumerate(pages):\n",
    "        text = pytesseract.image_to_string(page)\n",
    "        full_text += text + \"\\n\\n\"  # Add extra newlines between pages\n",
    "        print(f\"Processed page {i+1}/{len(pages)}\")\n",
    "        \n",
    "    # Save to text file\n",
    "    with open('atomic_habits.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(full_text)\n",
    "        \n",
    "    print(f\"Successfully saved text to atomic_habits.txt\")\n",
    "    print(f\"Total characters extracted: {len(full_text)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split text into 154 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 154/154 [12:31<00:00,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved cleaned text to atomic_habits_cleaned.txt\n",
      "Original length: 306614\n",
      "Cleaned length: 305269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "\n",
    "    client = openai.OpenAI(\n",
    "        # This is the default and can be omitted\n",
    "        api_key=os.getenv('OPENAI_TOKEN'),\n",
    "    )\n",
    "    \n",
    "    # Read the original text file\n",
    "    with open('atomic_habits.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Split into chunks of ~2000 characters to stay within API limits\n",
    "    chunk_size = 2000\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    \n",
    "    print(f\"Split text into {len(chunks)} chunks\")\n",
    "    \n",
    "    # Process each chunk with ChatGPT\n",
    "    cleaned_chunks = []\n",
    "    for chunk in tqdm(chunks, desc=\"Processing chunks\"):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that cleans up and reformats text. Fix any OCR errors, weird line breaks, and formatting issues to make the text clean and readable.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Please clean up and reformat this text:\\n\\n{chunk}\"}\n",
    "            ]\n",
    "        )\n",
    "        cleaned_chunks.append(response.choices[0].message.content)\n",
    "        \n",
    "    # Combine chunks and save to new file\n",
    "    cleaned_text = \"\\n\".join(cleaned_chunks)\n",
    "    \n",
    "    with open('atomic_habits_cleaned.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(cleaned_text)\n",
    "        \n",
    "    print(f\"Successfully saved cleaned text to atomic_habits_cleaned.txt\")\n",
    "    print(f\"Original length: {len(text)}\")\n",
    "    print(f\"Cleaned length: {len(cleaned_text)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the context file\n",
    "with open('my context.txt', 'r', encoding='utf-8') as f:\n",
    "    context = f.read()\n",
    "\n",
    "\n",
    "def get_prompt_chunk_rewriting(chunk):\n",
    "    return f\"\"\"\n",
    "    I will give a part of the Atomic Habits book. Please rewrite it in a way that is tailored to my own needs and my own examples. You don't need to change the overall gist, you just need to showcase relevant examples from my life.\n",
    "\n",
    "    Here is the part of the book:\n",
    "    {chunk}\n",
    "\n",
    "    Here is my context:\n",
    "    {context}\n",
    "\n",
    "    Please rewrite the text in a way that is tailored to my own needs and my own examples. You don't need to change the overall gist, you just need to showcase relevant examples from my life. And don't add anything else.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 153/153 [13:10<00:00,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved rewritten text to atomic_habits_rewritten_v0.txt\n",
      "Original length: 305757\n",
      "Rewritten length: 299304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Read the original cleaned text\n",
    "with open('atomic_habits_cleaned.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Split into chunks of ~2000 characters\n",
    "chunk_size = 2000\n",
    "chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Process each chunk with ChatGPT\n",
    "rewritten_chunks = []\n",
    "for chunk in tqdm(chunks, desc=\"Processing chunks\"):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that rewrites text to be more personalized.\"},\n",
    "                {\"role\": \"user\", \"content\": get_prompt_chunk_rewriting(chunk)}\n",
    "            ]\n",
    "        )\n",
    "        rewritten_chunks.append(response.choices[0].message.content)\n",
    "        time.sleep(1) # Rate limiting\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk: {str(e)}\")\n",
    "        rewritten_chunks.append(chunk) # Keep original on error\n",
    "        \n",
    "# Combine chunks and save to new file\n",
    "rewritten_text = \"\\n\".join(rewritten_chunks)\n",
    "\n",
    "with open('atomic_habits_rewritten_v0.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(rewritten_text)\n",
    "    \n",
    "print(f\"Successfully saved rewritten text to atomic_habits_rewritten_v0.txt\")\n",
    "print(f\"Original length: {len(text)}\")\n",
    "print(f\"Rewritten length: {len(rewritten_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
